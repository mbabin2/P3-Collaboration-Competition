{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathias Babin - P3 Collaboration and Competition Training\n",
    "\n",
    "This is my implementation for solving the P3 Collaboration-Competition project for [Udacity's Deep Reinforcement Learning course](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). Details on the project are provided in the **README** for this repository. The purpose of this notebook is to **train** an Agent to solve this environment. If you wish to watch a **finished** agent perform in this enviroment, please go to the **Collab-Test** notebook included in this repository.\n",
    "\n",
    "\n",
    "### 1. Setting up the Environment\n",
    "\n",
    "Running the following cell gaurentees that both [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/) have been installed correctly, along with several other packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from agent import Agent\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was built and tested on a 64-bit OSX system. To make this application run on a different OS please change the file path in the next cell to one of the following:\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "Note that all of these files **_should_** already be included in the repository as .zip files, simply extract the one that matches your current OS (OSX .app already extracted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell simply sets up the Enviroment. **_IMPORTANT:_**  If the following cell opens a Unity Window that crashes, this is because the rest of the cells in the project are not being executed fast enough. To avoid this, please select **Restart & Run All** under **Kernal**. This will execute all the cells in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the Agent\n",
    "\n",
    "Start by importing some necessary packages and intialize values for the training of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get brains from Unity ML\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "num_agents = len(env_info.agents) # get number of agents\n",
    "\n",
    "action_size = brain.vector_action_space_size # get action size\n",
    "\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1] # get state space size\n",
    "\n",
    "# Initialize the agents\n",
    "agents = Agent(state_size=state_size, action_size=action_size, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Agents for n episodes, and report its average score over 100 episodes. This environment is considered solved once the agent has maintained a score of +0.50 for atleast 100 episodes. Initially, the blue agent begins by taking only random actions inorder to add experiences to the shared replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beRandom = True # set to true if agent 2 should use random policy\n",
    "num_episodes = 1500 # number of episodes\n",
    "scores_avg = deque(maxlen=100) # average over 100 episodes\n",
    "all_scores = [] # scores used for visualization.\n",
    "\n",
    "for i in range(1, num_episodes+1): # loop over all episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations # Get initial states\n",
    "    scores = np.zeros(num_agents) # scores that each agent recieves\n",
    "    score = 0 # score for each episode\n",
    "\n",
    "    while True: # loop over all timesteps\n",
    "        action1 = agents.act(states[0]) # action for agent 1\n",
    "        action2 = agents.act(states[1]) # action for agent 2\n",
    "        actions = np.random.randn(num_agents, action_size) # randomized actions\n",
    "        actions = np.clip(actions, -1, 1) # clip random actions\n",
    "        actions[0] = action1 # replace random action with agent 1 action\n",
    "        if not beRandom:\n",
    "            actions[1] = action2 # replace random action with agent 2 action\n",
    "        elif np.mean(scores_avg) >= 0.05 and i >= 100 and beRandom: # if agent 1 has improved enough, switch agent 2 policy\n",
    "            beRandom = False\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name] # step in the environment\n",
    "        next_states = env_info.vector_observations # get next state\n",
    "        rewards = env_info.rewards # get rewards\n",
    "        dones = env_info.local_done # get if episode is done\n",
    "        scores += env_info.rewards # sum rewards as score\n",
    "        \n",
    "        # update NNs\n",
    "        agents.step(states[0], actions[0], rewards[0], next_states[0], dones[0]) # add agent 1 experiences to buffer\n",
    "        agents.step(states[1], actions[1], rewards[1], next_states[1], dones[1]) # add agent 2 experiences to buffer\n",
    "        \n",
    "        states = next_states  # prepare for next epsiode by setting a new state\n",
    "        if np.any(dones): # exit if done episode\n",
    "            break\n",
    "\n",
    "    score = np.max(scores) # score is largest of two agents\n",
    "    scores_avg.append(score) # average score over 100 episodes\n",
    "    all_scores.append(score) # keep track of all scores for graphs\n",
    "    if i > 1:\n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i, np.mean(scores_avg), score), end=\"\")\n",
    "    if i % 100 == 0:\n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}'.format(i, np.mean(scores_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training results of training (Score vs. Episode Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(all_scores)+1), all_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode Num')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the trained weights and close the environment down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation Details\n",
    "\n",
    "If you have any questions about the implementation details of this project please refer to the **Report.pdf** file included with this repository for a full explanation of both the algorithms and design decisions chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
